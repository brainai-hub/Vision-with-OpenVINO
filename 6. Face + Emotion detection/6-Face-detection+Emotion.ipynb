{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd17303d-755e-4980-8b6e-e6ca74be398c",
   "metadata": {},
   "source": [
    "# Face detection\n",
    "\n",
    "## please note: this notebook need your edit in order to work properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf378fd-3e6c-44ee-b1c6-3b1a0765e519",
   "metadata": {},
   "source": [
    "## 1. Get the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80f4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from openvino.runtime import Core\n",
    "\n",
    "sys.path.append(\"utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968bc251-16f2-4b79-b8ec-593dbb60d98a",
   "metadata": {},
   "source": [
    "## 2. Get the model\n",
    "\n",
    "the function \"get_model\" will\n",
    "1. Download the model from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)  \n",
    "\n",
    "2. Convert it to OpenVINO (IR) format, if needed..\n",
    "\n",
    "3. Create an OpenVINO, inference ready object based on that model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c009d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(base_model_dir,model_name,precision = 'FP16' ,sub_dir='intel'):\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {model_name} \" \\\n",
    "                   f\"--output_dir {base_model_dir} \" \\\n",
    "                   f\"--precisions {precision}\"\n",
    "    ! $download_command\n",
    "    \n",
    "    # The output path for the conversion.\n",
    "    converted_model_path = f\"{base_model_dir}/{sub_dir}/{model_name}/{precision}/{model_name}.xml\"\n",
    "\n",
    "    if not os.path.exists(converted_model_path):\n",
    "        convert_command = f\"omz_converter \" \\\n",
    "                          f\"--name {model_name} \" \\\n",
    "                          f\"--download_dir {base_model_dir} \" \\\n",
    "                          f\"--precisions {precision}\"\n",
    "        ! $convert_command\n",
    "        \n",
    "    # Initialize OpenVINO Runtime.\n",
    "    ie_core = Core()\n",
    "    \n",
    "    # Read the network and corresponding weights from a file.\n",
    "    model = ie_core.read_model(model=converted_model_path)\n",
    "    \n",
    "    # Compile the model for CPU \n",
    "    return ie_core.compile_model(model=model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25aba6-fb06-4d61-acae-a01ef0f91d76",
   "metadata": {},
   "source": [
    "## Face detection class.\n",
    "\n",
    "Create a class for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bad522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading face-detection-adas-0001 ||################\n",
      "\n",
      "========== Downloading face_detect\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.xml\n",
      "... 100%, 304 KB, 1295 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading face_detect\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.bin\n",
      "... 49%, 1024 KB, 2337 KB/s, 0 seconds passed\n",
      "... 99%, 2048 KB, 3195 KB/s, 0 seconds passed\n",
      "... 100%, 2056 KB, 3130 KB/s, 0 seconds passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class face_detection():\n",
    "    def __init__(self):\n",
    "        self.compiled_model = get_model('face_detect','face-detection-adas-0001')\n",
    "        \n",
    "        self.output_layer = self.compiled_model.output(0)\n",
    "        input_layer = self.compiled_model.input(0)\n",
    "        self.height, self.width = list(input_layer.shape)[2:4]\n",
    "face_det=face_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4136ae34-15c8-4e97-950d-1c457b231349",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3015092536.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    =====>>>  CREATE THE EMOTION DETECTION CLASS HERE...   <<<===========\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class emotion_detection():\n",
    "=====>>>  CREATE THE EMOTION DETECTION CLASS HERE...   <<<===========\n",
    "\n",
    "emotion_det=emotion_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9dbde-1f08-4570-b850-a4548e889797",
   "metadata": {},
   "source": [
    "## process the points (x,y) of the bounding boxes\n",
    "\n",
    "Correct small errors, stretch the box if needed\n",
    "Change box points from (x,y,dx,dy) to actuall cordinates (x,y upper left,   x,y lower right) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(box,bounds,stretch=0):\n",
    "    #the NN can sometimes return negative numbers that makes no sense \n",
    "    box=[max(x,0) for x in box]\n",
    "    \n",
    "    #getting points of the ends of the box (stretching a bit)\n",
    "    x1 = box[0] - stretch*box[2]\n",
    "    x2 = box[0] + (1+stretch)*box[2]\n",
    "    y1 = box[1] - stretch*box[3]\n",
    "    y2 = box[1] + (1+stretch)*box[3]\n",
    "    \n",
    "    #make sure that after streching, we are still in the image boundaries..\n",
    "    x1,x2=(int(min(max(x,0),bounds[1])) for x in (x1,x2))\n",
    "    y1,y2=(int(min(max(y,0),bounds[0])) for y in (y1,y2))\n",
    "    assert x1<=x2 and y1<=y2 \n",
    "    return x1,x2,y1,y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2fdf9-8ced-4c71-9b75-9651efd28f0e",
   "metadata": {},
   "source": [
    "### Process boxes..\n",
    "\n",
    "For each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(frame, results, thresh=0.1):\n",
    "    # The size of the original frame.\n",
    "    h, w = frame.shape[:2]\n",
    "    results = results.squeeze()\n",
    "    boxes = []\n",
    "    \n",
    "    for idx, label, confidence, xmin, ymin, xmax, ymax in results:\n",
    "        if label==1. and confidence>thresh:\n",
    "            # Create a box with pixels coordinates from the box with normalized coordinates [0,1].\n",
    "            boxes.append(\n",
    "                tuple(map(int, (xmin * w, ymin * h, (xmax - xmin) * w, (ymax - ymin) * h)))\n",
    "            )\n",
    "    return boxes\n",
    "\n",
    "EMOTION_NAMES=['neutral','happy','sad','surprise','anger']\n",
    "\n",
    "def process_faces(frame, boxes):\n",
    "    final_image=frame.copy()\n",
    "    color = (0,200,0)\n",
    "    for box in boxes:\n",
    "              \n",
    "        #Add another box on the final image\n",
    "        x1,x2,y1,y2=get_points(box,final_image.shape[:2])\n",
    "        cv2.rectangle(img=final_image, pt1=(x1,y1), pt2=(x2, y2), color=color, thickness=2)\n",
    "        \n",
    "        ##prepare input input image for emotion\n",
    "        emotion_input = frame[y1:y2,x1:x2]    #take only the face box from the original image   \n",
    "        \n",
    "        emotion_input = cv2.resize( src=emotion_input, dsize=(emotion_det.width, emotion_det.height))   #now the input is [64 64 3]    \n",
    "               \n",
    "        emotion_input = emotion_input[np.newaxis].transpose([0,3,1,2])           #now it is   [1, 3, 64, 64]\n",
    "        \n",
    "        #run emotion inference\n",
    "        emotion_output = emotion_det.compiled_model([emotion_input])[emotion_det.output_layer]\n",
    "        emotion_output = emotion_output.squeeze()\n",
    "        index=np.argmax(emotion_output)\n",
    "        index\n",
    "        \n",
    "        cv2.putText(\n",
    "            img=final_image,\n",
    "            text=f\"{' '}{EMOTION_NAMES[index]}\",\n",
    "            org=(box[0] + 10, box[1] + 30),\n",
    "            fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            fontScale=frame.shape[1] / 1000,\n",
    "            color=color,\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "        \n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f32f65-e950-4b0f-96b7-fb4fd3553fc9",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "\n",
    "Get a frame by frame, process it, display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing function to run object detection.\n",
    "def main_loop(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    player = None\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = utils.VideoPlayer(\n",
    "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
    "        )\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Resize the image and change dims to fit neural network input.\n",
    "            input_img = cv2.resize( src=frame, dsize=(face_det.width, face_det.height))\n",
    "            \n",
    "            # Create a batch of images (size = 1).\n",
    "            input_img = input_img[np.newaxis, ...]\n",
    "            input_img=np.transpose(input_img,[0,3,1,2])\n",
    "            \n",
    "            # performe inference, output is a ]1 1 200 7] (x,y are 0-1)\n",
    "            results = face_det.compiled_model([input_img])[face_det.output_layer]\n",
    "                   \n",
    "            # process the boxes, scale to full image, convert to x,y,dx,dy \n",
    "            boxes = get_boxes(frame=frame, results=results)\n",
    "            \n",
    "            # Draw boxes on a frame.\n",
    "            frame = process_faces(frame=frame, boxes=boxes)  \n",
    "\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                )\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4a248-3d3d-4cbb-bf6d-cb0a88a85b3f",
   "metadata": {},
   "source": [
    "### Run webCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_loop(source=0, flip=False, use_popup=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c85284-ea4f-4656-81e9-2be23cf6be46",
   "metadata": {},
   "source": [
    "### Run on local video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4d697-c699-4d4b-a101-d398f7433962",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file='videos/male-female-faces.mp4'\n",
    "main_loop(source=video_file, flip=False, use_popup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0353a2-2e8c-47ef-b3fb-9e0938d7d8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42503119-96b7-4dd7-9086-d6c332184ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
