{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd17303d-755e-4980-8b6e-e6ca74be398c",
   "metadata": {},
   "source": [
    "# Face detection + Emotion + Age + Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf378fd-3e6c-44ee-b1c6-3b1a0765e519",
   "metadata": {},
   "source": [
    "## 1. Get the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80f4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from openvino.runtime import Core\n",
    "\n",
    "sys.path.append(\"utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968bc251-16f2-4b79-b8ec-593dbb60d98a",
   "metadata": {},
   "source": [
    "## 2. Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c009d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(base_model_dir,model_name,precision = \"FP16\",sub_dir='intel'):\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {model_name} \" \\\n",
    "                   f\"--output_dir {base_model_dir} \" \\\n",
    "                   f\"--cache_dir {base_model_dir}\"\n",
    "    ! $download_command\n",
    "    # The output path for the conversion.\n",
    "    converted_model_path = f\"{base_model_dir}/{sub_dir}/{model_name}/{precision}/{model_name}.xml\"\n",
    "\n",
    "    if not os.path.exists(converted_model_path):\n",
    "        convert_command = f\"omz_converter \" \\\n",
    "                          f\"--name {model_name} \" \\\n",
    "                          f\"--download_dir {base_model_dir} \" \\\n",
    "                          f\"--precisions {precision}\"\n",
    "        ! $convert_command\n",
    "    # Initialize OpenVINO Runtime.\n",
    "    ie_core = Core()\n",
    "    # Read the network and corresponding weights from a file.\n",
    "    model = ie_core.read_model(model=converted_model_path)\n",
    "    return ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "#compiled_model=get_model('face_detect','face-detection-adas-0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e3d5a-7abe-4577-8171-128b402f57e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bad522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading face-detection-adas-0001 ||################\n",
      "\n",
      "========== Retrieving face_detect\\intel\\face-detection-adas-0001\\FP32\\face-detection-adas-0001.xml from the cache\n",
      "\n",
      "========== Retrieving face_detect\\intel\\face-detection-adas-0001\\FP32\\face-detection-adas-0001.bin from the cache\n",
      "\n",
      "========== Retrieving face_detect\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.xml from the cache\n",
      "\n",
      "========== Retrieving face_detect\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.bin from the cache\n",
      "\n",
      "========== Retrieving face_detect\\intel\\face-detection-adas-0001\\FP16-INT8\\face-detection-adas-0001.xml from the cache\n",
      "\n",
      "========== Retrieving face_detect\\intel\\face-detection-adas-0001\\FP16-INT8\\face-detection-adas-0001.bin from the cache\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class face_detection():\n",
    "    def __init__(self):\n",
    "        self.compiled_model = get_model('face_detect','face-detection-adas-0001')\n",
    "        \n",
    "        self.output_layer = self.compiled_model.output(0)\n",
    "        input_layer = self.compiled_model.input(0)\n",
    "        self.height, self.width = list(input_layer.shape)[2:4]\n",
    "face_det=face_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bdadc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading emotions-recognition-retail-0003 ||################\n",
      "\n",
      "========== Retrieving emotions_detect\\intel\\emotions-recognition-retail-0003\\FP32\\emotions-recognition-retail-0003.xml from the cache\n",
      "\n",
      "========== Retrieving emotions_detect\\intel\\emotions-recognition-retail-0003\\FP32\\emotions-recognition-retail-0003.bin from the cache\n",
      "\n",
      "========== Retrieving emotions_detect\\intel\\emotions-recognition-retail-0003\\FP16\\emotions-recognition-retail-0003.xml from the cache\n",
      "\n",
      "========== Retrieving emotions_detect\\intel\\emotions-recognition-retail-0003\\FP16\\emotions-recognition-retail-0003.bin from the cache\n",
      "\n",
      "========== Retrieving emotions_detect\\intel\\emotions-recognition-retail-0003\\FP16-INT8\\emotions-recognition-retail-0003.xml from the cache\n",
      "\n",
      "========== Retrieving emotions_detect\\intel\\emotions-recognition-retail-0003\\FP16-INT8\\emotions-recognition-retail-0003.bin from the cache\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class emotion_detction():\n",
    "    def __init__(self):\n",
    "        self.compiled_model = get_model('emotions_detect','emotions-recognition-retail-0003')\n",
    "        \n",
    "        self.output_layer = self.compiled_model.output(0)\n",
    "        input_layer = self.compiled_model.input(0)\n",
    "        self.height, self.width = list(input_layer.shape)[2:4]\n",
    "emotion_det=emotion_detction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3410ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading age-gender-recognition-retail-0013 ||################\n",
      "\n",
      "========== Retrieving age_gender_model\\intel\\age-gender-recognition-retail-0013\\FP32\\age-gender-recognition-retail-0013.xml from the cache\n",
      "\n",
      "========== Retrieving age_gender_model\\intel\\age-gender-recognition-retail-0013\\FP32\\age-gender-recognition-retail-0013.bin from the cache\n",
      "\n",
      "========== Retrieving age_gender_model\\intel\\age-gender-recognition-retail-0013\\FP16\\age-gender-recognition-retail-0013.xml from the cache\n",
      "\n",
      "========== Retrieving age_gender_model\\intel\\age-gender-recognition-retail-0013\\FP16\\age-gender-recognition-retail-0013.bin from the cache\n",
      "\n",
      "========== Retrieving age_gender_model\\intel\\age-gender-recognition-retail-0013\\FP16-INT8\\age-gender-recognition-retail-0013.xml from the cache\n",
      "\n",
      "========== Retrieving age_gender_model\\intel\\age-gender-recognition-retail-0013\\FP16-INT8\\age-gender-recognition-retail-0013.bin from the cache\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class age_gender_detection():\n",
    "    def __init__(self):\n",
    "        self.compiled_model = get_model('age_gender_model','age-gender-recognition-retail-0013')\n",
    "        \n",
    "        self.age = self.compiled_model.output(1)\n",
    "        self.gender = self.compiled_model.output(0)\n",
    "        input_layer = self.compiled_model.input(0)\n",
    "        self.height, self.width = list(input_layer.shape)[2:4]\n",
    "ag_det=age_gender_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b196d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(box,bounds,stretch=0):\n",
    "    #the NN can sometimes return negative numbers that makes no since \n",
    "    box=[max(x,0) for x in box]\n",
    "    \n",
    "    #getting points of the ends of the box (stretching a bit)\n",
    "    x1 = box[0] - stretch*box[2]\n",
    "    x2 = box[0] + (1+stretch)*box[2]\n",
    "    y1 = box[1] -stretch*box[3]\n",
    "    y2 = box[1] + (1+stretch)*box[3]\n",
    "    \n",
    "    x1,x2=(int(min(max(s,0),bounds[1])) for s in (x1,x2))\n",
    "    y1,y2=(int(min(max(s,0),bounds[0])) for s in (y1,y2))\n",
    "    assert x1<=x2 and y1<=y2 \n",
    "    return x1,x2,y1,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06f5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_faces(frame, boxes):\n",
    "    \n",
    "    for box in boxes:\n",
    "        ...\n",
    "         \n",
    "        #age-gender input\n",
    "        input_img=x[y1:y2,x1:x2]\n",
    "        input_img = cv2.resize(\n",
    "                src=input_img, dsize=(ag_det.width, ag_det.height),interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        \n",
    "        \n",
    "        input_img = input_img[np.newaxis]\n",
    "        input_img=np.transpose(input_img,[0,3,1,2])\n",
    "        \n",
    "        #age-gender output\n",
    "        output= ag_det.compiled_model([input_img])\n",
    "        age,gender=output[ag_det.age],output[ag_det.gender]\n",
    "        \n",
    "        age=np.squeeze(age)\n",
    "        age*=100\n",
    "        \n",
    "        gender=np.squeeze(gender)\n",
    "        if (gender[0]>=0.65):\n",
    "            gender='female '\n",
    "        elif (gender[1]>=0.55):\n",
    "            gender='male '\n",
    "        else:\n",
    "            gender='nb '\n",
    "        \n",
    "        #drawing results\n",
    "        cv2.putText(\n",
    "            img=frame,\n",
    "            text=f\"{gender}{age:.0f}{' '}{EMOTION_NAMES[index]}\", #{emotion_score:.0f}\n",
    "            org=(box[0] + 10, box[1] + 30),\n",
    "            fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            fontScale=frame.shape[1] / 1000,\n",
    "            color=color,\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf3abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(frame, results, thresh=0.1):\n",
    "    # The size of the original frame.\n",
    "    h, w = frame.shape[:2]\n",
    "    results = results.squeeze()\n",
    "    boxes = []\n",
    "    for _, label, score, xmin, ymin, xmax, ymax in results:\n",
    "        if label==1. and score>thresh:\n",
    "            # Create a box with pixels coordinates from the box with normalized coordinates [0,1].\n",
    "            boxes.append(\n",
    "                tuple(map(int, (xmin * w, ymin * h, (xmax - xmin) * w, (ymax - ymin) * h)))\n",
    "            )\n",
    "    return boxes\n",
    "\n",
    "\n",
    "EMOTION_NAMES=['neutral','happy','sad','surprise','anger']\n",
    "\n",
    "def process_faces(frame, boxes):\n",
    "    x=frame.copy()\n",
    "    for box in boxes:\n",
    "        #showing the base box\n",
    "        color = (0,200,0)\n",
    "        x1,x2,y1,y2=get_points(box,x.shape[:2])\n",
    "        cv2.rectangle(img=frame, pt1=(x1,y1), pt2=(x2, y2), color=color, thickness=2)\n",
    "        \n",
    "        #emotion  input\n",
    "        input_img=x[y1:y2,x1:x2]\n",
    "        input_img = cv2.resize(\n",
    "                src=input_img, dsize=(emotion_det.width, emotion_det.height),interpolation=cv2.INTER_AREA)\n",
    "        input_img = input_img[np.newaxis]\n",
    "        input_img=np.transpose(input_img,[0,3,1,2])\n",
    "        \n",
    "        #emotion output\n",
    "        emotion= emotion_det.compiled_model([input_img])[emotion_det.output_layer]\n",
    "        emotion=np.squeeze(emotion)\n",
    "        index=np.argmax(emotion)\n",
    "        emotion_score=emotion[index]\n",
    "             \n",
    "        #age-gender input\n",
    "        #input_img=x[y1:y2,x1:x2]\n",
    "        input_img=x[y1-10:y2+10,x1-10:x2+10]\n",
    "        \n",
    "        input_img = cv2.resize(\n",
    "                src=input_img, dsize=(ag_det.width, ag_det.height),interpolation=cv2.INTER_AREA )\n",
    "        input_img = input_img[np.newaxis]\n",
    "        input_img=np.transpose(input_img,[0,3,1,2])\n",
    "        \n",
    "        #age-gender output\n",
    "        output= ag_det.compiled_model([input_img])\n",
    "        age,gender=output[ag_det.age],output[ag_det.gender]\n",
    "        \n",
    "        age=np.squeeze(age)\n",
    "        age*=100\n",
    "        \n",
    "        gender=np.squeeze(gender)\n",
    "        if (gender[0]>=0.65):\n",
    "            gender='female '\n",
    "        elif (gender[1]>=0.55):\n",
    "            gender='male '\n",
    "        else:\n",
    "            gender='nb '\n",
    "        \n",
    "        #drawing results\n",
    "        cv2.putText(\n",
    "            img=frame,\n",
    "            text=f\"{gender}{age:.0f}{' '}{EMOTION_NAMES[index]}\", #{emotion_score:.0f}\n",
    "            org=(box[0] + 10, box[1] + 30),\n",
    "            fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            fontScale=frame.shape[1] / 1000,\n",
    "            color=color,\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0b2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing function to run object detection.\n",
    "def main_loop(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    player = None\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = utils.VideoPlayer(\n",
    "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
    "        )\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Resize the image and change dims to fit neural network input.\n",
    "            input_img = cv2.resize(\n",
    "                src=frame, dsize=(face_det.width, face_det.height)\n",
    "            )\n",
    "            # Create a batch of images (size = 1).\n",
    "            input_img = input_img[np.newaxis, ...]\n",
    "            input_img=np.transpose(input_img,[0,3,1,2])\n",
    "            \n",
    "            # Get the results.\n",
    "            results = face_det.compiled_model([input_img])[face_det.output_layer]\n",
    "            \n",
    "            # Get boxes from network results.\n",
    "            boxes = get_boxes(frame=frame, results=results)\n",
    "            \n",
    "            \n",
    "            # Draw boxes on a frame.\n",
    "            frame = process_faces(frame=frame, boxes=boxes)\n",
    "            \n",
    "\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                )\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2dd99",
   "metadata": {},
   "source": [
    "### run on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c09e394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "video_file='videos/walking-faces.mp4'\n",
    "main_loop(source=video_file, flip=False, use_popup=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d7bba",
   "metadata": {},
   "source": [
    "### run on a Web-Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0ef03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "main_loop(source=0, flip=False, use_popup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058be4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
